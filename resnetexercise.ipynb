{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercies has an implementation on an arbitrary length network defined by you the user. This network was written for \n",
    "$$ Y = w_n(cos(w_{n-1}*z_{n-1})) $$\n",
    "Where $$ z_{n-1} = (cos(w_{n-2}*z_{n-2})) $$\n",
    "starting with $z_0$\n",
    "This first block just initiallizes some random weights to train both the residual and nonresidual network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 3.  #Mapping X = input to Y = output\n",
    "Y = 6.\n",
    "\n",
    "numnodes = 50 # Sets the number of layers, this includes the input and output\n",
    "\n",
    "eta = 1.0e-4 #set stepsize for later grad descent\n",
    "\n",
    "nodeval = np.random.rand(numnodes) #initialize the node values with the first node as the input\n",
    "nodeval[0] = X\n",
    "\n",
    "weights = np.random.rand(numnodes-1) #initializes weights, and gradients, and bias\n",
    "nodegrad = np.random.rand(numnodes-1)\n",
    "bias = np.random.rand(numnodes-1)\n",
    "\n",
    "nodeval_1 = nodeval + 0 #There will be two tests, one with a plain network here\n",
    "weights_1 = weights + 0\n",
    "nodegrad_1 = nodegrad + 0\n",
    "bias_1 = nodegrad + 0 \n",
    "\n",
    "nodeval_res = nodeval + 0 #This will be for the resnet \n",
    "weights_res = weights + 0\n",
    "nodegrad_res = nodegrad + 0\n",
    "bias_res = bias + 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines forward and backward modes for these derivatives. I'm about 80 percent sure this is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(nodes, weights, bias): #define forward mode for the plain network\n",
    "\n",
    "    for i in range(len(nodes)-2):\n",
    "        #each layer is defined of the previous output times a weight plus a bias\n",
    "        nodenext = np.sin(weights[i]*nodes[i]+bias[i])\n",
    "        nodes[i+1] = nodenext\n",
    "    \n",
    "        grad = np.cos(weights[i]*nodes[i]+bias[i])*weights[i] #The gradient of each layer wrt the input\n",
    "        nodegrad[i+1] = grad\n",
    "    \n",
    "    nodes[-1] = weights[-1]*nodes[-2] + bias[-1] #set the last node with a linear transformation\n",
    "    nodegrad[-1] = weights[-1] #set that gradient wrt to input\n",
    "    \n",
    "    return nodes, nodegrad\n",
    "\n",
    "\n",
    "def backward(nodes, weights, bias, nodegrad, seed): #define backward mode\n",
    "    \n",
    "    gradientw = [] #save each weight and bias gradients\n",
    "    gradientb = []\n",
    "    \n",
    "    gradienttot = seed #seed is defined by the loss function, squared difference in this case\n",
    "    gradientw.append(gradienttot*nodes[-2]) #save the gradient wrt to first weight\n",
    "    gradientb.append(gradienttot)\n",
    "    \n",
    "    for i in range(len(nodegrad)-1):\n",
    "        gradienttot *= nodegrad[-(i+2)] #backpropogate \n",
    "        gradientw.append(gradienttot*nodes[-(3+i)]*np.cos(nodes[-(3+i)]*weights[-(2+i)]+bias[-(2+i)]))\n",
    "        gradientb.append(gradienttot*np.cos(nodes[-(3+i)]*weights[-(2+i)]+bias[-(2+i)]))\n",
    "        \n",
    "    gradientw = np.array(gradientw) #convert to arrays\n",
    "    gradientb = np.array(gradientb)\n",
    "    \n",
    "    return gradientw[::-1], gradientb[::-1] #flip because I appended the output gradient first\n",
    "\n",
    "def dL(out, target): #define loss function\n",
    "    L = (out - target)**2\n",
    "    dL = 2*(out - target)\n",
    "    return L, dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.43028140635458\n",
      "0.006444483290424556\n",
      "2.1130441959557825e-06\n",
      "6.932870026826189e-10\n",
      "2.2746918843647854e-13\n",
      "7.463335999766456e-17\n",
      "2.4486319115386765e-20\n",
      "8.632397600750422e-24\n",
      "1.232595164407831e-24\n",
      "1.232595164407831e-24\n",
      "Target =  6.0  Guess =  5.99999999999889\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000): #Train the network for a bunch of epochs\n",
    "    nodeval_1, nodegrad_1 = forward(nodeval_1, weights_1, bias_1)\n",
    "    \n",
    "    loss, lossgrad = dL(nodeval_1[-1], Y)\n",
    "    \n",
    "    gradientsw, gradientsb = backward(nodeval_1, weights_1, bias_1, nodegrad_1, lossgrad)\n",
    "    \n",
    "    weights_1 -= gradientsw*eta\n",
    "    bias_1 -= gradientsb*eta\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print(loss)\n",
    "\n",
    "nodeval_1, nodegrad_1 = forward(nodeval_1, weights_1, bias_1) #compute final guess\n",
    "\n",
    "print('Target = ', Y, ' Guess = ', nodeval_1[-1]) #See how well we did\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that isn't the best. Let's try a residual implementation where $$ Y = w_n(cos(w_{n-1}*z_{n-1})) + z_{n-1}$$\n",
    "Where $$ z_{n-1} = (cos(w_{n-2}*z_{n-2})) + z_{n-2} $$\n",
    "starting with $z_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_res(nodes, weights, bias): #same architecture but with skip connections\n",
    "\n",
    "    for i in range(len(nodes)-2):\n",
    "        nodenext = np.sin(weights[i]*nodes[i]+bias[i]) + nodes[i] #here the input is added\n",
    "        nodes[i+1] = nodenext\n",
    "    \n",
    "        grad = np.cos(weights[i]*nodes[i])*weights[i] + 1 #derivative wrt to the input is 1 so that is added\n",
    "        nodegrad[i+1] = grad\n",
    "    \n",
    "    nodes[-1] = weights[-1]*nodes[-2] + nodes[-2] + bias[-1]\n",
    "    nodegrad[-1] = weights[-1] + 1\n",
    "    \n",
    "    \n",
    "    return nodes, nodegrad\n",
    "\n",
    "\n",
    "def backward_res(nodes, weights, bias, nodegrad, seed): #backpropogate this function is the same in both modes\n",
    "    \n",
    "    gradientw = []\n",
    "    gradientb = []\n",
    "    \n",
    "    gradienttot = seed\n",
    "    gradientw.append(gradienttot*nodes[-2])\n",
    "    gradientb.append(gradienttot)\n",
    "    \n",
    "    for i in range(len(nodegrad)-1):\n",
    "        gradienttot *= nodegrad[-(i+2)]\n",
    "        gradientw.append(gradienttot*nodes[-(3+i)]*np.cos(nodes[-(3+i)]*weights[-(2+i)]+bias[-(2+i)]))\n",
    "        gradientb.append(gradienttot*np.cos(nodes[-(3+i)]*weights[-(2+i)]+bias[-(2+i)]))\n",
    "        \n",
    "    gradientw = np.array(gradientw)\n",
    "    gradientb = np.array(gradientb)\n",
    "    \n",
    "    return gradientw[::-1], gradientb[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.50191573055269\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "1.4586038137536508e-27\n",
      "Target =  6.0  Guess =  6.000000000000038\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000): #train the network\n",
    "    nodeval_res, nodegrad_res = forward_res(nodeval_res, weights_res, bias_res)\n",
    "    \n",
    "    loss, lossgrad = dL(nodeval_res[-1], Y)\n",
    "\n",
    "    gradientsw, gradientsb = backward_res(nodeval_res, weights_res, bias_res, nodegrad_res, lossgrad)\n",
    "    \n",
    "    weights_res -= gradientsw*eta\n",
    "    bias_res -= gradientsb*eta\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print(loss)\n",
    "        \n",
    "nodeval_res, nodegrad_res = forward_res(nodeval_res, weights_res, bias_res) #compute final answer\n",
    "print('Target = ', Y, ' Guess = ', nodeval_res[-1]) #see how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "check = np.linspace(0, 60, 1000) #Plots the output functions as a function of the input, you can see how the\n",
    "#mappings change\n",
    "empty = np.zeros(numnodes)\n",
    "values = []\n",
    "\n",
    "for i in check:\n",
    "    empty[0] = i\n",
    "    n, g = forward_res(empty, weights_res, bias_res)\n",
    "    values.append(n[-1])\n",
    "\n",
    "plt.plot(check, values)\n",
    "\n",
    "values = []\n",
    "\n",
    "for i in check:\n",
    "    empty[0] = i\n",
    "    n, g = forward(empty, weights_1, bias_1)\n",
    "    values.append(n[-1])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(check, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming, I implemented this correctly, just increadible. It is interesting to node that the plain network tries to supress the sin, and just move with the bias, where the residual mapping, has a sort of step function to work with, and converges a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was trying to get this to work with T-money's autodiff, but was failing. It would be more general and fun if we can get it working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Constant:\n",
    "    \n",
    "    def __init__(self, a):\n",
    "        # Initialize the value of this variable with the value passed in\n",
    "        self.a_value = a\n",
    "        # to make sure our children don't throw an exception when they access our grad variable \n",
    "        # as in backward() in the BinaryAdd()\n",
    "        self.grad = 0\n",
    "    \n",
    "    def forward(self):\n",
    "        # what should this return?\n",
    "        # TODO: fill in the return value\n",
    "        # Remember this is the last node of the graph.\n",
    "        return self.a_value\n",
    "    \n",
    "    def backward(self):\n",
    "        # What should go here if it's a constant?\n",
    "        # Nothing. We want the backpropagation to stop here. \n",
    "        # In python, we use pass as shorthand for \"return None\".\n",
    "        # Doing this means we can inject constants at any point in the computational graph.\n",
    "        pass\n",
    "\n",
    "class BinaryAdd:\n",
    "    \n",
    "    def __init__(self, a, b):\n",
    "        # record the two parents of the binary add\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        # and initialize the gradient to 0.\n",
    "        self.grad = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        # a _value and b_value\n",
    "        # are intermediate values in the computational graph\n",
    "        # like v4 in Table 3 in the paper.\n",
    "        # We don't have to store the value of a or b, \n",
    "        # but caching them now means we don't have to recompute them on the backward pass.\n",
    "        self.a_value = self.a.forward()\n",
    "        self.b_value = self.b.forward()\n",
    "        return self.a_value + self.b_value\n",
    "    \n",
    "    def backward(self):\n",
    "        # z = a + b\n",
    "        # dz/da = ?\n",
    "        # TODO: fill in the None values.\n",
    "        # Remember, a and b are the parents of this object. \n",
    "        dzda = 1\n",
    "        dzdb = 1\n",
    "        self.a.grad += dzda*self.grad\n",
    "        self.b.grad += dzdb*self.grad\n",
    "\n",
    "class BinaryMul:\n",
    "    \n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.grad = 0\n",
    "        # TODO: what's missing? What other value do we have to store in a node?\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        # again, we don't have to cache self.a_value or self.b_value\n",
    "        # but it makes the backward pass not have to call a.forward() or b.forward()\n",
    "        # TODO: fill in None\n",
    "        self.a_value = self.a.forward()\n",
    "        self.b_value = self.b.forward()\n",
    "        return self.a_value * self.b_value\n",
    "\n",
    "    def backward(self):\n",
    "        # TODO: fill in the gradient values below (dzda, dzdb)\n",
    "        # z = a*b \n",
    "        # dz/da = ?\n",
    "        dzda = self.b_value\n",
    "        dzdb = self.a_value\n",
    "        self.a.grad += dzda*self.grad\n",
    "        self.b.grad += dzdb*self.grad\n",
    "        \n",
    "class Ln:\n",
    "    \n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "        self.grad = 0\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO: fill me in\n",
    "        self.a_value = self.a.forward()\n",
    "        return np.log(self.a_value)\n",
    "    \n",
    "    def backward(self):\n",
    "        # TODO: fill me in\n",
    "        # z = ln(a)\n",
    "        # dz/da = ?\n",
    "        self.a.grad += 1/self.a_value*self.grad\n",
    "\n",
    "        \n",
    "class Sin:\n",
    "    \n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "        self.grad = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO: fill me in\n",
    "        self.a_value = self.a.forward()\n",
    "        return np.sin(self.a_value)\n",
    "    \n",
    "    def backward(self):\n",
    "        # TODO: fill me in \n",
    "        # z = sin(a)\n",
    "        # dz/da = ?\n",
    "        self.a.grad += np.cos(self.a_value)*self.grad\n",
    "\n",
    "\n",
    "class BinarySub:\n",
    "    \n",
    "    def __init__(self, a, b):\n",
    "        # TODO:\n",
    "        # record the two parents of the binary subtract\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        # and initialize the gradient to 0.\n",
    "        self.grad = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO: How do I get a and b?\n",
    "        self.a_value = self.a.forward()\n",
    "        self.b_value = self.b.forward()\n",
    "        return self.a_value - self.b_value\n",
    "    \n",
    "    def backward(self):\n",
    "        # z = a - b\n",
    "        # dz/da = ?\n",
    "        # TODO: fill in the None values.\n",
    "        # Remember, a and b are the parents of this object. \n",
    "        dzda = 1\n",
    "        dzdb = -1\n",
    "        self.a.grad += dzda*self.grad\n",
    "        self.b.grad += dzdb*self.grad  # Why is this a += operator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: -0.9785235488295365\n"
     ]
    }
   ],
   "source": [
    "X = Constant(3)\n",
    "Y = 2.\n",
    "\n",
    "w1 = Constant(.4)\n",
    "w2 = Constant(.1)\n",
    "\n",
    "v1 = BinaryMul(X, w1)\n",
    "v2 = Sin(v1)\n",
    "v3 = BinaryMul(v2, w2)\n",
    "\n",
    "eta = 1.0e-5\n",
    "\n",
    "for i in range(100000):\n",
    "\n",
    "    y = v3.forward()\n",
    "    \n",
    "    #print('y: {}'.format(y))\n",
    "    #print(.5*np.sin(.4*4))\n",
    "\n",
    "    # TODO: seed the gradient value\n",
    "    v3.grad = 2*(y - Y) \n",
    "    v3.backward()\n",
    "\n",
    "    # Instead of calling backward() on each node by hand, put the nodes in \n",
    "    # a list and iterate over it backwards.\n",
    "\n",
    "    ls = [X, w1, v1, v2, w2]\n",
    "\n",
    "    for node in ls[::-1]:\n",
    "        #print(node.grad)\n",
    "        node.backward()\n",
    "    \n",
    "    w1.a_value -= w1.grad*eta\n",
    "    w2.a_value -= w2.grad*eta\n",
    "\n",
    "print('y: {}'.format(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,layers, input_size, num_class):\n",
    "        super(Net, self).__init__()\n",
    "        self.input = input_size\n",
    "        self.classes = num_class\n",
    "        self.layers = layers\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input,self.input)])\n",
    "        self.linears.extend([nn.Linear(self.input, self.input) for i in range(1, self.layers-1)])\n",
    "        self.linears.append(nn.Linear(self.input, self.classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        temp = None\n",
    "        i = 0\n",
    "        for layer in self.linears:\n",
    "            if i%3 == 0:\n",
    "                x = torch.sigmoid(layer(x))\n",
    "                i += 1\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                i += 1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784       # The image size = 28 x 28 = 784\n",
    "num_classes = 10       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 10         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 1e-3  # The speed of convergence\n",
    "net = Net(10, input_size,num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn.ParameterList(net.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3143715858459473 10.279999732971191\n",
      "1 2.302456855773926 11.350000381469727\n",
      "2 2.3005683422088623 11.350000381469727\n",
      "3 2.2983646392822266 11.350000381469727\n",
      "4 2.2984426021575928 11.350000381469727\n",
      "5 2.316406726837158 11.350000381469727\n",
      "6 2.3070859909057617 11.350000381469727\n",
      "7 2.293673038482666 11.350000381469727\n",
      "8 2.3094537258148193 10.279999732971191\n",
      "9 2.2959601879119873 11.350000381469727\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        images = Variable(images.view(-1,28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()\n",
    "    total=0\n",
    "    correct=0\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = net(d.view(-1,28*28))\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += Variable(t).size(0)\n",
    "        correct += (predicted==t).sum()\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),(100.*correct/total).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resNet(nn.Module):\n",
    "    def __init__(self,layers, input_size, num_class):\n",
    "        super(resNet, self).__init__()\n",
    "        self.input = input_size\n",
    "        self.classes = num_class\n",
    "        self.layers = layers\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input,self.input)])\n",
    "        self.linears.extend([nn.Linear(self.input, self.input) for i in range(1, self.layers-1)])\n",
    "        self.linears.append(nn.Linear(self.input, self.classes))\n",
    "    def forward(self, x):\n",
    "        temp = x\n",
    "        i = 0\n",
    "        for layer in self.linears:\n",
    "            print(layer(x))\n",
    "            if i%3 == 0:\n",
    "                x = torch.sigmoid(layer(x))\n",
    "                i += 1\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                i += 1\n",
    "        print(x.shape, temp.shape)\n",
    "        return x + temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784       # The image size = 28 x 28 = 784\n",
    "num_classes = 10       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 10         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 1e-3  # The speed of convergence\n",
    "resnet = resNet(10, input_size,num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn.ParameterList(net.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0603, -0.2374, -0.2596,  ..., -0.0049, -0.0785,  0.1213],\n",
      "        [ 0.0942,  0.3139,  0.1319,  ...,  0.0852,  0.2276,  0.1503],\n",
      "        [-0.1532,  0.4307,  0.0131,  ...,  0.1803,  0.0502,  0.3117],\n",
      "        ...,\n",
      "        [ 0.2572,  0.1046, -0.0641,  ..., -0.0265,  0.0656,  0.1438],\n",
      "        [-0.1443, -0.0782, -0.0690,  ...,  0.2466, -0.0380, -0.2145],\n",
      "        [ 0.0371,  0.3303, -0.0805,  ...,  0.1398, -0.0016,  0.3181]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2547,  0.3964, -0.0981,  ...,  0.3998,  0.0472,  0.4862],\n",
      "        [ 0.2870,  0.4213, -0.1102,  ...,  0.4171,  0.0543,  0.4647],\n",
      "        [ 0.2867,  0.4680, -0.1283,  ...,  0.3914,  0.0279,  0.5167],\n",
      "        ...,\n",
      "        [ 0.2586,  0.4255, -0.0690,  ...,  0.4227,  0.0383,  0.4575],\n",
      "        [ 0.2364,  0.4134, -0.1002,  ...,  0.3563,  0.0842,  0.4902],\n",
      "        [ 0.2969,  0.4835, -0.1125,  ...,  0.4052,  0.0309,  0.4820]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2523,  0.4345,  0.0059,  ..., -0.1859, -0.0016, -0.1131],\n",
      "        [ 0.2739,  0.4161, -0.0037,  ..., -0.1653, -0.0163, -0.1400],\n",
      "        [ 0.2777,  0.4486, -0.0034,  ..., -0.1966,  0.0061, -0.1267],\n",
      "        ...,\n",
      "        [ 0.2682,  0.4348,  0.0168,  ..., -0.1503, -0.0130, -0.1156],\n",
      "        [ 0.2620,  0.4338,  0.0119,  ..., -0.1736,  0.0291, -0.1475],\n",
      "        [ 0.2971,  0.4526, -0.0029,  ..., -0.1705,  0.0165, -0.1348]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.0088,  0.0203,  0.0949,  ..., -0.0391,  0.1194, -0.0668],\n",
      "        [-0.0270,  0.0192,  0.0955,  ..., -0.0296,  0.1337, -0.0751],\n",
      "        [-0.0146,  0.0211,  0.1191,  ..., -0.0420,  0.1226, -0.0583],\n",
      "        ...,\n",
      "        [-0.0168,  0.0195,  0.0818,  ..., -0.0339,  0.1275, -0.0698],\n",
      "        [-0.0104,  0.0324,  0.0926,  ..., -0.0389,  0.1172, -0.0629],\n",
      "        [-0.0284,  0.0148,  0.1036,  ..., -0.0335,  0.1253, -0.0650]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4002,  0.4940,  0.1087,  ..., -0.0360,  0.3859, -0.1391],\n",
      "        [ 0.3999,  0.4954,  0.1074,  ..., -0.0360,  0.3850, -0.1392],\n",
      "        [ 0.4010,  0.4935,  0.1088,  ..., -0.0348,  0.3858, -0.1392],\n",
      "        ...,\n",
      "        [ 0.3988,  0.4947,  0.1078,  ..., -0.0370,  0.3869, -0.1395],\n",
      "        [ 0.4012,  0.4933,  0.1073,  ..., -0.0369,  0.3867, -0.1398],\n",
      "        [ 0.4018,  0.4927,  0.1082,  ..., -0.0380,  0.3870, -0.1397]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.0094, -0.0067, -0.0439,  ...,  0.0250,  0.1100,  0.1977],\n",
      "        [-0.0086, -0.0075, -0.0418,  ...,  0.0247,  0.1101,  0.1982],\n",
      "        [-0.0099, -0.0060, -0.0434,  ...,  0.0254,  0.1090,  0.1967],\n",
      "        ...,\n",
      "        [-0.0091, -0.0082, -0.0420,  ...,  0.0254,  0.1107,  0.1979],\n",
      "        [-0.0084, -0.0061, -0.0437,  ...,  0.0258,  0.1092,  0.1978],\n",
      "        [-0.0090, -0.0059, -0.0437,  ...,  0.0261,  0.1102,  0.1981]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1231,  0.0941, -0.1110,  ..., -0.2785,  0.0304,  0.0944],\n",
      "        [ 0.1235,  0.0932, -0.1110,  ..., -0.2793,  0.0308,  0.0945],\n",
      "        [ 0.1235,  0.0939, -0.1106,  ..., -0.2780,  0.0294,  0.0950],\n",
      "        ...,\n",
      "        [ 0.1228,  0.0938, -0.1113,  ..., -0.2796,  0.0308,  0.0949],\n",
      "        [ 0.1230,  0.0937, -0.1105,  ..., -0.2784,  0.0302,  0.0948],\n",
      "        [ 0.1233,  0.0930, -0.1110,  ..., -0.2789,  0.0299,  0.0956]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2688, -0.2011, -0.5498,  ...,  0.2207, -0.1673,  0.0115],\n",
      "        [ 0.2687, -0.2011, -0.5499,  ...,  0.2206, -0.1673,  0.0115],\n",
      "        [ 0.2687, -0.2011, -0.5498,  ...,  0.2207, -0.1673,  0.0115],\n",
      "        ...,\n",
      "        [ 0.2688, -0.2012, -0.5499,  ...,  0.2206, -0.1673,  0.0115],\n",
      "        [ 0.2687, -0.2011, -0.5498,  ...,  0.2207, -0.1673,  0.0115],\n",
      "        [ 0.2686, -0.2011, -0.5499,  ...,  0.2207, -0.1674,  0.0115]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2137, -0.0904, -0.0042,  ...,  0.1073,  0.0867, -0.1064],\n",
      "        [-0.2137, -0.0904, -0.0042,  ...,  0.1074,  0.0867, -0.1063],\n",
      "        [-0.2137, -0.0904, -0.0042,  ...,  0.1073,  0.0867, -0.1064],\n",
      "        ...,\n",
      "        [-0.2137, -0.0905, -0.0042,  ...,  0.1074,  0.0867, -0.1064],\n",
      "        [-0.2137, -0.0904, -0.0042,  ...,  0.1072,  0.0867, -0.1064],\n",
      "        [-0.2137, -0.0904, -0.0042,  ...,  0.1073,  0.0867, -0.1063]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0922],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0922],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0577, -0.0393, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0393, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0577, -0.0393, -0.0785, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1004,  0.0922],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1010,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1004,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0453,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0922],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0393, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0393, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0922],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0922],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0830, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0922],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0577, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921],\n",
      "        [-0.1097, -0.0831, -0.1010,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0922],\n",
      "        [-0.1097, -0.0831, -0.1011,  0.0460, -0.0576, -0.0394, -0.0784, -0.0452,\n",
      "         -0.1003,  0.0921]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([100, 10]) torch.Size([100, 784])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (784) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2b75e7a44a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# Intialize the hidden weight to all zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# Forward pass: compute the output class given a image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# Compute the loss: difference between the output class and the pre-given label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                   \u001b[0;31m# Backward pass: compute the weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-b31b61dc25cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (784) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        images = Variable(images.view(-1,28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = resnet(images)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()\n",
    "    total=0\n",
    "    correct=0\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = net(d.view(-1,28*28))\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += Variable(t).size(0)\n",
    "        correct += (predicted==t).sum()\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),(100.*correct/total).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
